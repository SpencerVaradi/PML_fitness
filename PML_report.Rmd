---
title: "PML Fitness"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Libraries and load Data
```{r cache=TRUE}
library(caret)
library(ggplot2)
library(rpart)
library(corrplot)
library(reshape2)
library(randomForest)
library(e1071)
library(MASS)

training_origin <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", header = TRUE) #, stringsAsFactors = FALSE)
training_origin <- training_origin[,-1]
testing_origin <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", header = TRUE) #, stringsAsFactors = FALSE)
testing_origin <- testing_origin[,-1]
```
##Create training and testing subsets
```{r cache=TRUE}
set.seed(500)
inTrain <- createDataPartition(training_origin$classe, p = 3/4)[[1]]

training <- training_origin[inTrain,]
testing <- training_origin[-inTrain,]
trainingClasse <- training$classe
```
Classe was removed from the testing set to keep the predictors clean and match with training set. The set was also cleaned of extra columns which were not present in the training set.
```{r cache=TRUE}
# Hold onto the classe before we chuck it
testingClasse <- testing$classe
testingAnswers <- data.frame(cbind(problem_id = 1:dim(testing)[1], classe = testingClasse))
# Get rid of columns that we do not have the luxery of in the testing data
# testing <- testing[,names(testing) %in% names(testing_origin)]
# Since we will be pulling the testing set apart a little bit, IDs will make rejoining easier
testing$problem_id <- 1:dim(testing)[1]
```

## Cleaning training set
Most of the columns were numeric or factors. Others were more information about the record. These were identified so they could be added to and subset of the training data. Of the remaining columns, many were missing most values. Those empty columns were identified and removed.
```{r cache=TRUE}
# Identify cols that are not measures. More record info.
recCols <- c("user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window")

# Remove the columns which are mostly empty which tosses out most of the columns
findEmpty <- function(x){
  sum(length(which(is.na(x) | x == "" | is.null(x))))
}

thresh <- .95

percentEmpty <- sapply(training, findEmpty)/dim(training)[1]
rmColNames <- names(percentEmpty)[percentEmpty >= thresh]
trainingClean <- training[,-which(names(training) %in% rmColNames)]
```
# Classes
Classes are almost equally distributed
```{r cache=TRUE}
qplot(trainingClean$classe)
```

# Machine Learning Methods
## Linear Discriminant Analysis (LDA)
LDA produced predictions with 82.1% accuracy. Classe B was the most difficult to predict and commonly confused with A and C.
```{r cache=TRUE}
set.seed(500)
ldaFit <- lda(classe ~ ., data = trainingClean)
ldaPred <- predict(ldaFit, newdata = testing)
confusionMatrix(ldaPred$class, testingClasse)
```

## Random Forrest (RF)
RF model produced 99.99% accuracy. Only 2 classes were misclassified.
When this was submitted, it yeilded 17 out of 20 correct predictions. With an 85% accuracy actually closer to the LDA model, the model is either missing something or overfit to the training data. the randomForest() function appears to have a pretty significant bug when it comes using predict(). The model has to be fit from a subset the same dataframe.
```{r cache=TRUE}

# randomForest functin is not working properly within Knitr. So this was done in R session, saved, and loaded

# rfFit <- randomForest(classe ~ ., data = data.frame(trainingAll[nrow(trainingClean),], classe = trainingClean$classe))
# saveRDS(rfFit, rfFit.rds)
rfFit <- readRDS("rfFit.rds")
rfPred <- predict(rfFit, newdata = testing)
confusionMatrix(rfPred, testing$classe)
```

